# 2.8 PyTorch模型保存与读取

在PyTorch中，保存和加载模型对于深度学习的实践至关重要。这不仅关乎于将模型的知识和学习成果长期存储下来，也涉及到将模型从一个环境迁移到另一个环境、在训练过程中的状态恢复，以及模型的共享与复用。为了帮助你更好地掌握这些技能，我们在这节课将提供PyTorch模型保存和加载的实用指南。

经过本节的学习，你将收获：

- 了解和使用模型的`state_dict()`
- 掌握`torch.save()`和`torch.load()`函数的使用
- 学会在单卡与多卡训练环境下保存和加载模型
- 了解如何保存更多状态和完成断点续训

## 2.8.1 什么是state_dict()

回顾之前的章节，我们通过继承`torch.nn.Module`类，成功构建了一个简单的神经网络。在PyTorch中，一个模型由两部分组成：模型结构与参数权重。模型结构是通过类定义的，而参数权重则被存储在一个称为`state_dict()`的特殊字典中，其中键值对表示层的名称和对应的参数。

`state_dict()`可以被看作是模型的智能核心。如果用大脑来比喻一个模型，`state_dict()`中的键（key）就好比大脑的不同区域，而它的值（value）则代表那些区域存储的记忆和知识。`state_dict()`本质上是一个封装了所有学习参数的容器，打开它，你会发现数字层层叠叠，这些数字代表了模型在学习过程中所积累的权重值。

当我们需要将训练好的模型保存起来，无论是为了将来的使用，还是在其他设备上进行部署，我们都会保存这个`state_dict()`。因为它包含了模型最关键的信息，所以当我们在未来某个时刻加载这个`state_dict()`时，模型就能够回忆起之前学到的一切，恢复到之前的学习状态。

我们将通过PyTorch官方提供的[例子](https://pytorch.org/tutorials/beginner/saving_loading_models.html#what-is-a-state-dict)，帮你更好的了解`state_dict()`

```python
# 导入必要的包
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F

# 定义模型
class CustomModel(nn.Module):
    def __init__(self):
        super(CustomModel, self).__init__()
        # 定义第一个卷积层，输入通道为3，输出通道为6，卷积核大小为5
        self.conv1 = nn.Conv2d(3, 6, 5)
        # 定义一个池化层，使用最大池化，池化窗口大小为2，步幅为2
        self.pool = nn.MaxPool2d(2, 2)
        # 定义第二个卷积层，输入通道为6，输出通道为16，卷积核大小为5
        self.conv2 = nn.Conv2d(6, 16, 5)
        # 定义第一个全连接层，输入特征为16*5*5，输出特征为120
        self.fc1 = nn.Linear(16 * 5 * 5, 120)
        # 定义第二个全连接层，输入特征为120，输出特征为84
        self.fc2 = nn.Linear(120, 84)
        # 定义第三个全连接层，输入特征为84，输出特征为10
        self.fc3 = nn.Linear(84, 10)

    def forward(self, x):
        # 通过第一个卷积层后，使用ReLU激活函数，再进行池化
        x = self.pool(F.relu(self.conv1(x)))
        # 通过第二个卷积层后，使用ReLU激活函数，再进行池化
        x = self.pool(F.relu(self.conv2(x)))
        # 将特征图展平，准备连接全连接层
        x = x.view(-1, 16 * 5 * 5)
        # 通过第一个全连接层后，使用ReLU激活函数
        x = F.relu(self.fc1(x))
        # 通过第二个全连接层后，使用ReLU激活函数
        x = F.relu(self.fc2(x))
        # 通过第三个全连接层获得最终输出
        x = self.fc3(x)
        return x

# 初始化模型
model = CustomModel()

# 初始化优化器，这里使用随机梯度下降优化器，学习率设置为0.001，动量为0.9
optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)

# 打印模型的state_dict
print("Model's state_dict:")
for param_tensor in model.state_dict():
    # 打印参数名称和对应的维度
    print(param_tensor, "\t", model.state_dict()[param_tensor].size())

# 打印优化器的state_dict
print("Optimizer's state_dict:")
for var_name in optimizer.state_dict():
    # 打印优化器的属性名称和对应的值
    print(var_name, "\t", optimizer.state_dict()[var_name])

```

```python
Model's state_dict:
conv1.weight     torch.Size([6, 3, 5, 5]) # conv1.weight是key，torch.Size([6, 3, 5, 5])是对应的value的shape
conv1.bias   torch.Size([6])
conv2.weight     torch.Size([16, 6, 5, 5])
conv2.bias   torch.Size([16])
fc1.weight   torch.Size([120, 400])
fc1.bias     torch.Size([120])
fc2.weight   torch.Size([84, 120])
fc2.bias     torch.Size([84])
fc3.weight   torch.Size([10, 84])
fc3.bias     torch.Size([10])

Optimizer's state_dict:
state    {}
param_groups     [{'lr': 0.001, 'momentum': 0.9, 'dampening': 0, 'weight_decay': 0, 'nesterov': False, 'params': [4675713712, 4675713784, 4675714000, 4675714072, 4675714216, 4675714288, 4675714432, 4675714504, 4675714648, 4675714720]}]
```

在上面的代码中，我们定义了一个简单的卷积神经网络模型`CustomModel`和一个优化器实例，用于更新模型的权重。并且展示了如何通过打印模型的`state_dict`和优化器的`state_dict`来查看模型参数和优化器状态的详细信息。`state_dict`是一个从参数名称（键）映射到参数张量（值）的Python字典对象。

下面，我们将继续讲解如何保存和加载模型的`state_dict`，以及如何保存和加载完整的模型。

## 2.8.2 模型的保存和读取

一个PyTorch模型主要包含两个部分：模型结构和权重。其中模型是继承`nn.Module`的类，权重的数据结构是一个字典（key是层名，value是权重向量）。存储也由此分为两种形式：存储整个模型（包括结构和权重）和只存储模型权重（存储`state_dict()`）。

### 只保存和加载模型的参数

当我们只想保存模型的参数时，我们可以使用`state_dict()`去得到参数的权重并且保存，主要是以下代码：

```python
# torch.save(model.state_dict(), PATH) # 保存模型的参数
torch.save(model.state_dict(), './model_weights.pth') # 保存模型参数
```

这行代码将模型的参数保存到一个名为`model_weights.pth`的文件中。我们只保存了模型的权重，没有保存模型的结构。这样，当我们需要重新使用这个模型时，我们需要首先重建模型结构，然后再加载权重。

```python
# 加载模型参数
model = CustomModel()  # 需要先创建一个模型的实例
model.load_state_dict(torch.load('./model_weights.pth')) # 将参数load进入模型的state_dict
model.eval()  # 将模型设置为评估模式，这通常在推理之前需要调用
```

### 保存和加载完整模型

除了保存模型的参数之外，我们还可以保存整个模型，包括其结构和参数。以下是保存整个模型的代码：

```python
# 保存整个模型
torch.save(model, './complete_model.pth')
```

这行代码将完整的模型保存到一个名为`complete_model.pth`的文件中。这包括了模型的结构和参数。因此，当我们想要加载模型时，我们不需要再重新创建模型结构。

加载整个模型的代码如下：

```python
# 加载整个模型
model = torch.load('./complete_model.pth')
model.eval()  # 同样地，将模型设置为评估模式
```

在直觉上, 我们认为保存一个完整的模型会更加方便。但是实际上PyTorch官方推荐的做法是仅保存模型的`state_dict`，主要是由于以下的几个原因：

1. **依赖于Python的`pickle`模块**：保存整个模型通常依赖于Python的`pickle`模块，这可能会在不同的Python版本或环境之间引发兼容性问题。特别是，如果在一个Python环境中保存模型，在一个不同版本的环境中加载它，可能会遇到难以预测的错误。
2. **代码变更问题**：保存整个模型会将模型结构和参数一起保存。如果后续修改了模型的结构代码，那么加载保存的模型时会出现问题，因为模型的定义可能已经不同了。
3. **模型的可移植性**：保存模型的`state_dict`而不是整个模型，可以更容易地将模型迁移到其他代码库或项目中，只要模型结构保持一致即可。
4. **文件大小**：保存整个模型通常会产生更大的文件，因为它包含了完整的类定义和参数。而保存`state_dict`只包含参数数据，因此文件更小，加载也更快。

总的来说就是保存模型参数的这种方法不仅更加灵活和可靠，而且在将来需要调整、优化模型结构时更加方便。当需要共享或部署模型时，通常只需要提供模型的代码和`state_dict`即可。这样，其他人可以使用自己的PyTorch环境来重建模型，并加载我们提供的权重。

由于`torch.load()`默认在cpu上加载，而我们一般在GPU上训练和保存模型，所以不注意设备问题就有可能报错。此时，需要使用`map_location`来将存储动态重新映射到可选设备上，比如`map_location=torch.device('cpu')`，意思是映射到cpu上，在cpu上加载模型，无论你这个模型从哪里训练保存的。

### 保存和加载多个模型

### 加载部分参数作为模型的热启动

### 其他参数的保存和读取

在深度学习项目里，有时候我们不仅仅需要保存模型的权重，还需要保存一些其他的参数，比如训练的epoch数、训练的loss，优化器的参数，动态调整学习策略的参数等等。这些参数可以通过字典的形式保存在一个文件里，然后在读取模型时一起读取。这里我们以下方代码为例：

```python
torch.save({
        'model': model.state_dict(),
        'optimizer': optimizer.state_dict(),
        'lr_scheduler': lr_scheduler.state_dict(),
        'epoch': epoch,
    	'loss': loss,
        'args': args,
    }, checkpoint_path)
```

这些参数的读取方式也是类似的：

```python
model = CustomModel() 
optimizer = CustomOptimizer()
lr_scheduler = CustomLrScheduler()

checkpoint = torch.load(checkpoint_path)

model.load_state_dict(checkpoint['model'])
optimizer.load_state_dict(checkpoint['optimizer'])
lr_scheduler.load_state_dict(checkpoint['lr_scheduler'])
epoch = checkpoint['epoch']
loss = checkpoint['loss']
args = checkpoint['args']

model.eval() # evaluation
model.train() # resume training
```
### 跨设备保存和加载模型
很多时候，我们需要跨设备保存和加载模型，一般基于以下的考虑：
- 可移植性：将模型保存为可在不同设备上加载和使用的格式，可以使模型具有更大的可移植性。这意味着您可以在不同的硬件平台、操作系统或编程语言上使用相同的模型进行推理或训练。

- 分布式训练和推理：在分布式系统中，模型的训练和推理可能涉及多个设备或计算节点。通过跨设备保存和加载模型，可以方便地在不同的节点之间传递模型参数，实现分布式训练和推理的协同工作。

- 模型共享和部署：保存模型并在不同设备上加载，可以方便地共享和部署模型。例如，我们可以将训练好的模型部署到移动设备、边缘设备或云服务器上，以满足不同应用场景的需求。

- 跨平台兼容性：不同的深度学习框架和库可能使用不同的模型表示和存储格式。通过跨设备进行模型的保存和加载，可以实现不同框架之间的互操作性，使得模型可以在不同的框架和库中使用。

跨设备保存一般有以下的几种情况，在此一一进行列举和代码的展示。
1. 保存在GPU，在CPU上加载  

保存：
```py
torch.save(model.state_dict(), PATH)
```

加载：
```py
device = torch.device('cpu')
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location=device))
```

2. 保存在GPU上，在另一个GPU上加载

保存：
```py
torch.save(model.state_dict(), PATH)
```

加载：
```py
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH))
model.to(device)
```

3. 保存在CPU上，在GPU上加载

保存：
```py
torch.save(model.state_dict(), PATH)
```

加载：
```py
device = torch.device("cuda")
model = TheModelClass(*args, **kwargs)
model.load_state_dict(torch.load(PATH, map_location="cuda:0"))  # Choose whatever GPU device number you want
model.to(device)
# Make sure to call input = input.to(device) on any input tensors that you feed to the model
```
更多细节可以参考官网的介绍[SAVING AND LOADING MODELS](https://pytorch.org/tutorials/beginner/saving_loading_models.html)。

## 参考资料

1. [PyTorch saving loading models](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
2. [What is the difference between .pt, .pth and .pwf extentions in PyTorch?](https://stackoverflow.com/questions/59095824/what-is-the-difference-between-pt-pth-and-pwf-extentions-in-pytorch)
3. [pytorch(一)模型加载函数torch.load()](https://blog.csdn.net/pearl8899/article/details/109566084)
4. [SAVING AND LOADING MODELS](https://pytorch.org/tutorials/beginner/saving_loading_models.html)
5. ChatGPT