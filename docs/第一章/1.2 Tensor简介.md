# 1.2 Tensor简介

在PyTorch中，我们通常将数据以张量的形式进行表示，张量可以看作是多维数组，它能够在GPU上运行，支持自动求导，是构建神经网络模型和进行数值计算的基本单位。比如我们用三维张量表示一个RGB图像，四维张量表示视频。因此在本章我们将介绍深度学习最基础的元素-张量，我们主要介绍**张量的创建，属性，运算，索引和切片，维度变换，广播和数据格式间的转化**等操作。

经过本节的学习，你将收获：

- 张量的简介
- 张量的创建和属性
- 张量的索引和切片
- 张量的维度变化

## 1.2.1 张量的简介

我们可以将张量理解为是多维数组或矩阵的一个抽象概念，在深度学习中，张量是一个可以在GPU上运算的多维数组。我们列举了一些常见的张量的例子，帮助大家更好的理解张量的含义。

|张量维度|代表含义|
|---|---|
| 0维张量 | 代表的是标量（数字） |
| 1维张量 | 代表的是向量 -> (1,N)/(N,1)|
| 2维张量 | 代表的是矩阵 -> (row,col)|
| 3维张量 | 单张彩色图片(**RGB**) -> (width, height, channel)|
| 4维张量 | 批量彩色图片(**RGB**) -> (batch, width, height, channel)|

在PyTorch中， `torch.Tensor` 是存储和变换数据的主要工具。如果你之前用过`NumPy`，你会发现 `Tensor` 和`NumPy`的多维数组非常类似。然而，`Tensor` 提供GPU计算和自动求梯度等更多功能，这些使 `Tensor` 这一数据类型更加适合深度学习。


## 1.2.2 创建tensor
在接下来的内容中，我们将介绍几种常见的创建`tensor`的方法。
- 基础的构建方式：`torch.tensor()` / `torch.from_numpy()`
- 特殊矩阵的构建：
  - 构建空的矩阵
  - 指定值矩阵的构建
    - 全0矩阵的构建：`torch.zeros()`
    - 全1矩阵的构建：`torch.ones()`
    - 指定值填充矩阵的构建：`torch.full()`
  - 对角矩阵的构建：`torch.eye()` / `torch.diag()`
  - 特定分布矩阵的构建：`torch.rand()` / `torch.randn()` / `torch.normal()`
  - 等间隔矩阵的构建：`torch.arange()` / `torch.linspace()`

### 基础构建方式

1. `torch.tensor()`构建张量
   我们可以通过`torch.tensor()`直接使用现有的数据，构造一个张量：

   ```python
   import torch  
   # 创建一维张量  
   x = torch.tensor([1, 2, 3])  
   # 创建二维张量  
   y = torch.tensor([[1, 2, 3], [4, 5, 6]])  
   # 创建三维张量
   z = torch.tensor([  
       [[1, 2, 3], [4, 5, 6]],  
       [[7, 8, 9], [10, 11, 12]],  
       [[13, 14, 15], [16, 17, 18]]  
   ])  
   print(x) # torch.Size([3])
   print(y) # torch.Size([2, 3])
   print(z) # torch.Size([3, 2, 3])
   ```

2. 从现有的Numpy数组构建张量

   我们可以使用`torch.from_numpy()`函数从现有的NumPy数组创建张量。这个函数返回的张量和原始的NumPy数组共享内存，所以改变其中一个会影响到另一个。

   ```python
   import numpy as np  
   import torch  
   # 创建一个NumPy数组  
   np_array = np.array([1, 2, 3])  
   # 从NumPy数组创建张量  
   x = torch.from_numpy(np_array)  
   print(x)  
   ```

   ```python
   tensor([1, 2, 3], dtype=torch.int32)
   ```

   如果我们想修改张量，但不想改变原始的NumPy数组，我们同样可以使用`torch.tensor()`函数来创建张量，这会复制数据，而不是共享内存。

   ```python
   import numpy as np  
   import torch  
   # 创建一个NumPy数组  
   np_array = np.array([1, 2, 3])  
   # 从NumPy数组创建张量  
   x = torch.tensor(np_array)  
   print(x)  
   ```

   ```python
   tensor([1, 2, 3], dtype=torch.int32)
   ```

### 指定值矩阵的构建

1. `torch.zeros()`构建全0矩阵
   我们可以通过`torch.zeros()`构造一个矩阵全为 0。

   ```python
   import torch
   x = torch.zeros(4, 3)
   y = torch.zeros(4)
   print(x)
   print(y)
   ```

   ```python
   tensor([[0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.],
           [0., 0., 0.]])
   tensor([0., 0., 0., 0.])
   ```

   除此以外，我们还可以通过`torch.zero_()`和`torch.zeros_like()`将现有矩阵转换为全0矩阵。

2. `torch.ones()`构建全1矩阵

   ```python
   import torch
   x = torch.ones(4, 3)
   y = torch.ones(4)
   print(x)
   print(y)
   ```

   ```python
   tensor([[1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.],
           [1., 1., 1.]])
   tensor([1., 1., 1., 1.])
   ```

3. `torch.ones()`构建指定值填充的矩阵

   我们可以通过指定值，创建一个全是指定值的矩阵。

   ```python
   import torch
   x = torch.full((2, 3), 3.5) # 矩阵的size必须是tuple
   y = torch.full((4,),4)
   print(x)
   print(y)
   ```

   ```python
   tensor([[3.5000, 3.5000, 3.5000],
           [3.5000, 3.5000, 3.5000]])
   tensor([4, 4, 4, 4])
   ```

### 对角矩阵的构建

1. `torch.eye()`构建对角线矩阵

   返回一个2D，对角线为1，其他位置为0的矩阵。

   ```python
   import torch
   x = torch.eye(3)
   print(x)
   ```

   ```python
   tensor([[1., 0., 0.],
           [0., 1., 0.],
           [0., 0., 1.]])
   ```

2. `torch.diag()`构建对角线为指定值的矩阵/取出对角线元素

   ```python
   import torch
   diag_values = torch.tensor([1, 2, 3, 4])  
   x = torch.diag(diag_values)  
   print(x)
   ```

   ```python
   tensor([[1, 0, 0, 0],
           [0, 2, 0, 0],
           [0, 0, 3, 0],
           [0, 0, 0, 4]])
   ```

   当我们有了一个矩阵，想取出对角线元素时，也可以使用`torch.diag()`取出对角线元素。

   ```python
   import torch
   x = torch.randn(3, 3)  # 创建 4*3 的正态分布矩阵
   diag_values = torch.diag(x)
   print(x)
   print(diag_values)
   ```

   ```python
   tensor([[ 0.9959, -1.9983, -1.7097],
           [-0.5075, -0.0265, -0.4691],
           [-2.0335,  1.0371,  0.1624]])
   tensor([ 0.9959, -0.0265,  0.1624])
   ```

### 特定分布矩阵的构建

1. `torch.rand()`构建随机初始化矩阵

   `torch.rand()` 会创建一个[0,1)区间的随机数矩阵，该矩阵的大小由我们初始化时决定。

   ```python
   import torch
   x = torch.rand(4, 3)  # 创建 4*3 的随机数矩阵
   y = torch.rand(4) # 创建向量
   print(x)
   print(y)
   ```

   ```python
   tensor([[0.7076, 0.8701, 0.7019],
           [0.4954, 0.9692, 0.3905],
           [0.4461, 0.1113, 0.5476],
           [0.0779, 0.8748, 0.0568]])
   tensor([0.0857, 0.3714, 0.8072, 0.1086])
   ```

2. `torch.randn()`构建随机正态分布矩阵

   `torch.rand()` 会创建一个符合均值为0，方差为1的正态分布的随机数矩阵，该矩阵的大小由我们初始化时决定。

   ```python
   import torch
   x = torch.randn(4, 3)  # 创建 4*3 的正态分布矩阵
   y = torch.randn(4) # 创建向量
   print(x)
   print(y)
   ```

   ```python
   tensor([[ 1.0007,  0.6869,  0.1667],
           [ 0.1286,  0.4498,  0.3239],
           [-1.4077,  0.4726,  0.3206],
           [ 0.3665,  1.3574, -0.7020]])
   tensor([ 2.0367, -0.8406, -0.9451, -1.4735])
   ```

3. `torch.normal()`构建符合多个正态分布/特定正态分布的矩阵

   `torch.normal()` 有多种用法，具体的用法可以查看PyTorch关于`torch.normal()`的[文档](https://pytorch.org/docs/stable/generated/torch.normal.html)，我们这里只举例创建一个符合指定均值和方差的正态分布的随机数矩阵，该矩阵的大小由我们初始化时决定。

   ```python
   import torch
   x = torch.normal(mean=2, std=3, size=(1, 4)) # 均值为2，方差为3
   ```

   ```python
   tensor([[ 2.1037,  4.1702, -1.2382,  1.0738]])
   ```

### 等间隔矩阵的构建

1. `torch.arange()`创建一个包含在区间 `[start, end)` 内以 `step` 为步长的序列的一维张量

   ```python
   import torch  
   # 使用 torch.arange()  
   x = torch.arange(0, 10, 2)  
   print(x)  
   ```

   ```python
   tensor([0, 2, 4, 6, 8])  
   ```

2. `torch.linspace(start, end, steps)`创建一个一维张量，其中包含 `steps` 个在闭区间 `[start, end]` 内均匀间隔的点。

   ```python
   import torch  
   # 使用 torch.linspace()  
   x = torch.linspace(0, 10, 5)  
   print(x)  
   ```

   ```python
   tensor([ 0.0000,  2.5000,  5.0000,  7.5000, 10.0000])
   ```

## 1.2.3 张量的属性

每一个张量都包含多个属性，考虑到实用性，我们只在这里对[`torch.dtype`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)和[`torch.device`](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device) 进行一个简单的介绍。关于更多的张量的属性(`torch.layout`和`torch.memory_format`)，我们可以查看[tensor_attributes](https://pytorch.org/docs/stable/tensor_attributes.html)进行学习和理解。

### torch.dtype

`torch.dtype`表示的是我们存储`torch.Tensor`时的数据类型，PyTorch有12种不同的数据类型，包括但不限于：

- `torch.float32` 或 `torch.float`：32位浮点数。
- `torch.float64` 或 `torch.double`：64位双精度浮点数。
- `torch.complex64`或`torch.cfloat`：64位复数
- `torch.complex128`或`torch.cdouble`：128位复数
- `torch.float16` 或 `torch.half`：16位半精度浮点数，使用1个符号位，5个指数位，10个有效位
- `torch.bfloat16`：16位半精度浮点数，使用1个符号位，8个指数位，7个有效位 （关于float16和bf16的区别我们会在后面的混合精度训练介绍）
- `torch.int32` 或 `torch.int`：32位整数。
- `torch.int64` 或 `torch.long`：64位整数。
- `torch.int16` 或 `torch.short`：16位整数。
- `torch.int8`：8位整数。
- `torch.uint8`：8位无符号整数。
- `torch.bool`：布尔值。

我们也可以在初始化张量时，就指定张量数据类型：

```python
import torch

int32_matrix = torch.zeros([2, 4], dtype=torch.int32)
# tensor([[ 0,  0,  0,  0],
#         [ 0,  0,  0,  0]], dtype=torch.int32)

float64_matrix = torch.ones([2, 4], dtype=torch.float64)
# tensor([[ 1.0000,  1.0000,  1.0000,  1.0000],
#         [ 1.0000,  1.0000,  1.0000,  1.0000]], dtype=torch.float64)
```

### torch.device

`torch.device` 是一个表示将张量分配到的设备的对象。设备可以是 cpu，cuda（如果我们有 NVIDIA GPU 并安装了 CUDA）或者mps。`torch.device` 可以让我们编写设备无关的代码，这意味着我们的代码可以在没有任何修改的情况下在 CPU 和 GPU 之间移植。但是需要注意的是，我们的代码应该减少在不同的设备之间的切换，减少不必要的消耗。

```python
import torch

# 创建一个设备对象，表示使用 CPU
device_cpu = torch.device('cpu')

# 创建一个设备对象，表示使用第 0 号 GPU
device_gpu = torch.device('cuda:0')

# 创建一个设备对象，表示使用Mac M系列的芯片
device_mps = torch.device('mps')

# 创建一个张量，默认分配到 CPU
tensor_cpu = torch.tensor([1.0, 2.0, 3.0])

# 创建一个张量，并将其分配到 GPU
tensor_gpu = torch.tensor([1.0, 2.0, 3.0], device=device_gpu)

# 也可以使用 `.to` 方法将张量移动到不同的设备
tensor_cpu_to_gpu = tensor_cpu.to(device_gpu)
```

## 1.2.4 张量的索引和切片

> 张量的索引和切片和python本身的索引和切片本身并无较大的差别，如果你熟悉python的索引和切片，可以跳过这部分内容

我们首先需要创建一个张量，我们可以使用上面讲述的张量的创建的方法进行创建，但是为了大家更加具体直观的了解，我们使用`torch.tensor()`来创建张量。

```python
import torch

# 创建一个二维张量，形状为(3,3)
tensor_2d = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])

# 创建一个三维张量，形状为(2, 3, 4)
tensor_3d = torch.tensor([[[1, 2, 3, 4], [5, 6, 7, 8], [9, 10, 11, 12]],
                          [[13, 14, 15, 16], [17, 18, 19, 20], [21, 22, 23, 24]]])

print(tensor_2d)
print(tensor_3d)
```

```python
tensor([[1, 2, 3],
        [4, 5, 6],
        [7, 8, 9]])

tensor([[[ 1,  2,  3,  4],
         [ 5,  6,  7,  8],
         [ 9, 10, 11, 12]],

        [[13, 14, 15, 16],
         [17, 18, 19, 20],
         [21, 22, 23, 24]]])
```

创建完张量后，我们将通过下面的代码块讲述如何获取单个元素，每一列，每一行，特定行列的子集等操作

1. 取值操作

   如果我们有一个元素 `tensor` ，我们只想获得它的数值结果，而不包含其他属性，比如梯度，我们可以使用 `.item()` 来获得这个张量的 `value`，而不获得其他性质：

   ```python
   import torch
   x = torch.randn(1) 
   print(type(x)) 
   print(type(x.item()))
   ```

   ```python
   <class 'torch.Tensor'>
   <class 'float'>
   ```

2. 获取特定元素

   ```python
   # 获取2d张量，3d张量的第一个元素
   element_2d = tensor_2d[0][0]
   element_3d = tensor_3d[0][0][0]
   ```

   ```python
   # element_2d
   tensor(1)
   # element_3d
   tensor(1)
   ```

3. 获取指定的行/列

   ```python
   # 获取2d张量的第一行
   row_2d = tensor_2d[0,:] # 选择第0行，所有的列元素，我们也可以使用row_2d = tensor_2d[0]
   # 获取2d张量的第一列
   col_2d = tensor_2d[:,0] # 选择第0行列，所有的行元素
   
   # 获取3d张量的[1,2,3,4]
   b0_row_3d = tensor_3d[0,0,:] # b0_row_3d = tensor_3d[0][0]
   # 获取3d张量的[13,14,15,16]
   b1_row_3d = tensor_3d[1,0,:] # b0_row_3d = tensor_3d[1][0]
   
   # 获取3d张量的[1,5,9]
   b0_col_3d = tensor_3d[0,:,0]
   # 获取3d张量的[13,17,21]
   b1_col_3d = tensor_3d[1,:,0]
   ```

   需要注意的是：索引出来的结果与原数据共享内存，修改一个，另一个会跟着修改。如果不想修改，可以考虑使用`clone()`等方法去解决

   ```python
   import torch
   x = torch.rand(4,3)
   # 取第二列
   print(x[:, 1]) 
   ```

   ```python
   tensor([-0.0720,  0.0666,  1.0336, -0.6965])
   ```

   对原始的数据进行索引：

   ```python
   y = x[0,:]
   y += 1
   print(y)
   print(x[0, :]) # 源tensor也被改了了
   ```

   ```python
   tensor([3.7311, 0.9280, 1.2497])
   tensor([3.7311, 0.9280, 1.2497])
   ```

   当我们使用`clone()`的方法时，可以解决这个问题。

   ```python
   import torch
   
   # 创建原始张量
   original_tensor = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32, requires_grad=True)
   
   # 使用索引选择元素，并创建一个副本，同时脱离原始张量的计算图
   indexed_elements = original_tensor[1:4].detach().clone()
   
   # 修改索引后的元素
   indexed_elements[0] = 10
   
   # 打印原始张量和修改后的索引元素
   print("Original tensor:", original_tensor)
   print("Indexed elements:", indexed_elements)
   ```

   ```python
   Original tensor: tensor([1, 2, 3, 4, 5]) # 原始的数据没有发生变化
   Indexed elements: tensor([10,  3,  4])
   ```

   要注意的是，`.clone()` 创建的是一个深度副本，`clone()` 后的张量会被记录在计算图中，即梯度回传到副本时也会传到源 Tensor 。所以它会占用额外的内存。在处理大型数据集时，这可能会成为一个考虑因素。只有在我们确实需要保持原始数据不变的情况下，才使用 `.clone()` 方法。除了使用 `.clone()` 方法创建张量副本外，我们也可以使用 `.detach()` 方法结合 `.clone()` 来创建一个完全脱离计算图的副本。对于一般的张量副本，`.clone()` 已经足够了，但如果我们只想要复制一个张量并且不想保留梯度信息，我们可以使用 `.detach().clone()`。

4. 获取指定的子集元素

   ```python
   # 基本切片：获取子集 [[5,6],[8,9]]
   subset_2d = tensor_2d[1:, 1:]
   # 获取特定行和列的子集
   subset_2d_2 = tensor_2d[:2, 1:] # 获取第0，1行，1，2列的元素子集
   
   # 基本三维切片，获取子集
   subset_3d_1 = tensor_3d[0, :, :] # 获取第一个维度为0的所有元素
   
   subset_3d_2 = tensor_3d[:, 1, :] # 获取第一个维度所有，第二个维度为1，第三个维度的所有子集
   
   subset_3d_3 = tensor_3d[:, :, 2] # 获取第三个维度为2的子集，第一个维度和第二个维度的所有元素
   ```

   ```python
   # subset_2d 
   tensor([[5, 6],
           [8, 9]])
   
   # subset_2d_2
   tensor([[2, 3],
           [5, 6]])
   
   # subset_3d_1
   tensor([[ 1,  2,  3,  4],
           [ 5,  6,  7,  8],
           [ 9, 10, 11, 12]])
   
   # subset_3d_2
   tensor([[ 5,  6,  7,  8],
           [17, 18, 19, 20]])
   
   # subset_3d_3
   tensor([[ 3,  7, 11],
           [15, 19, 23]])
   ```

## 1.2.5 张量的维度变化

1. 改变形状：`reshape()`和`view()`

   张量的维度变换常见的方法有`torch.view()`和`torch.reshape()`，下面我们将介绍第一中方法`torch.view()`：

   ```python
   import torch
   x = torch.randn(4, 4)
   y = x.view(16)
   z = x.view(-1, 8) # -1是指这一维的维数由其他维度决定
   print(x.size(), y.size(), z.size())
   ```

   ```python
   torch.Size([4, 4]) torch.Size([16]) torch.Size([2, 8])
   ```

   需要注意的是，`torch.view()` 返回的新`tensor`与源`tensor`共享内存(其实是同一个`tensor`)，更改其中的一个，另外一个也会跟着改变。(顾名思义，view()仅仅是改变了对这个张量的观察角度)，我们将通过下面的代码帮你解释这一情况。

   ```python
   x += 1
   print(x)
   print(y) # 也加了了1
   ```

   ```python
   tensor([[ 1.3019,  0.3762,  1.2397,  1.3998],
           [ 0.6891,  1.3651,  1.1891, -0.6744],
           [ 0.3490,  1.8377,  1.6456,  0.8403],
           [-0.8259,  2.5454,  1.2474,  0.7884]])
   tensor([ 1.3019,  0.3762,  1.2397,  1.3998,  0.6891,  1.3651,  1.1891, -0.6744,
            0.3490,  1.8377,  1.6456,  0.8403, -0.8259,  2.5454,  1.2474,  0.7884])
   ```

   上面我们说过`torch.view()`会改变原始张量，但是很多情况下，我们希望原始张量和变换后的张量互相不影响。为为了使创建的张量和原始张量不共享内存，我们需要使用第二种方法`torch.reshape()`， 同样可以改变张量的形状，但是此函数的返回值受到输入的张量的内存布局影响，并不能保证返回的是其拷贝值，所以官方不推荐使用。推荐的方法是我们先用 `clone()` 创造一个张量副本然后再使用 `torch.view()`进行函数维度变换 。

2. 移除维度/增加维度：`squeeze()`和`unsuqeeze()`

   当我们想要移除所有长度为1的维度，或者在特定位置增加一个长度为1的维度时，`squeeze()` 和 `unsqueeze()` 是非常有用的一种方法，这种可以很快的帮我们消除一个维度的存在或者把我们的数据构造成符合推理时的构成。

   ```python
   import torch
   
   # 创建张量
   x = torch.randn(2, 1, 2, 1, 2)
   
   # 移除所有长度为1的维度
   y = x.squeeze()
   
   # 只移除第1维的长度为1的维度
   z = x.squeeze(1) 
   
   # 在第1维增加一个长度为1的维度
   a = y.unsqueeze(1)
   print(x.size(), y.size(), z.size(), a.size())
   # x -> torch.Size([2, 1, 2, 1, 2]) 
   # t -> torch.Size([2, 2, 2]) 
   # z -> torch.Size([2, 2, 1, 2]) 
   # a -> torch.Size([2, 1, 2, 2])
   ```

3. 维度的交换：`transpose()`, `permute()` 和 `einops`

   我们可以理解深度学习实际上是很多矩阵之间的运算，在矩阵运算时，我们可能为了数值计算的方便就会对我们的矩阵进行维度的变换让他符合矩阵的计算的远离。当我们需要交换张量中两个维度的位置时，可以使用 `transpose()` 方法，而 `permute()` 方法可以更灵活地重新排列多个维度的顺序。

   ```python
   import torch
   
   # 创建张量
   x = torch.randn(2, 3, 5)
   
   # 交换第0维和第1维
   y = x.transpose(0, 1) 
   
   # 将第2维移到前面，重新排列维度，里面的索引对应还未移动前各元素的位置
   z = x.permute(2, 0, 1) 
   print(x.size(), y.size(), z.size())
   # x -> torch.Size([2, 3, 5]) 
   # y -> torch.Size([3, 2, 5]) 
   # z -> torch.Size([5, 2, 3])
   ```

   尽管`permute()`给我们提供了一种简介的变换维度的方法，但是我们需要提前知道矩阵的各个维度的数值和索引，才能去调换，为了更好的完成矩阵的变化，[arogozhnikov](https://github.com/arogozhnikov)等人开发了`einops`这个仓库，我们可以以一种更加优雅的方式去完成矩阵各个维度的变换，关于`einops`的操作我们可以查看：[Github](https://github.com/arogozhnikov/einops) 进行学习和操作，在本课程的后面的部分我们将会使用`einops`完成矩阵维度的变化。

4. 维度的扩展：`expand()` 和 `repeat()`

   当我们想要快速扩充现有的张量的时候，我们可以使用`expand()`和`repeat()`复制张量的元素来扩展其维度。`expand()`实际上不会实际复制数据，它返回的是一个视图。在扩展张量的尺寸有一定的限制，只能用来扩展尺寸为1的维度或者保持原有尺寸不变，而不能用来减少或者增加非1的维度大小。当我们使用`expand()`指定这个维度的值为-1时，代表我们不变化这个维度

   ```python
   import torch
   
   x = torch.tensor([[1], [2], [3]])
   
   # 查看张量的size() ->  torch.Size([3, 1])
   tensor_size = x.size()
   
   # 将x扩展到3，4 -> 维持第一个维度3不变，扩充第二个维度从1到6
   y = x.expand(3, 4) 
   
   z = x.expand(-1, 4) # 沿第二维度复制4次
   print(x.size(), y.size(), z.size())
   ```

   相比之下，`repeat()` 方法用于按照指定的次数复制张量的元素，它会实际复制数据，并创建一个新的张量。因此，使用 `repeat()` 会占用更多的内存。当我们调用一个张量的 `repeat()` 方法时，我们需要传递一个或多个整数，这些整数指定了每个维度复制的次数。传递给 `repeat()` 方法的参数是一个维度大小的序列，其中每个数字表示对应维度上数据的重复次数。

   ```python
   import torch
   
   # 创建张量
   x = torch.tensor([[1], [2], [3]])
   
   # 将x的第一个维度(行)重复2遍，将x的第二个维度(列)重复3遍
   y = x.repeat(2, 3)
   
   print(x.size(),y.size())
   # x -> torch.Size([3, 1]) 
   # y -> torch.Size([6, 3])
   ```



## 参考资料

1. [PyTorch tensor](https://pytorch.org/docs/stable/tensors.html#torch.Tensor)：PyTorch官方文档
2. [einops Github](https://github.com/arogozhnikov/einops)：einops官方仓库
3. ChatGPT
