# 2.2 数据读入

就像我们在引言所说的一样，在深度学习中，高效地读入和处理数据是至关重要的。PyTorch 通过 Dataset 和 DataLoader 类提供了一个强大且灵活的框架，以支持复杂数据集的快速迭代。Dataset 负责定义数据的格式和预处理方式，而 DataLoader 负责以迭代的方式批量读入数据，为模型训练提供动力。

经过本节的学习，你将收获：

- PyTorch常见的数据读取方式
- 构建自己的数据读取流程

## 使用官方提供的标准数据集

在许多情况下，我们不需要从头开始定义一个 `Dataset` 类，因为 PyTorch 提供了一些预定义的标准数据集，例如 CIFAR10, MNIST, Fashion-Mnist，ImageNet 等。这些计算机视觉的数据集已经集成在 `torchvision.datasets` 中，可以非常方便地使用。

以fashion mnist数据集为例，我们可以这样加载他：

```python
# 导入 PyTorch 和 torchvision 中必要的包
import torch
from torch.utils.data import Dataset
from torchvision import datasets
from torchvision.transforms import ToTensor
import matplotlib.pyplot as plt

# 加载 FashionMNIST 训练数据集
train_data = datasets.FashionMNIST(
    root="data",          # 指定存储数据集的目录，如果数据集不存在则会下载到这个目录
    train=True,           # 表示我们正在加载训练集部分
    download=True,        # 如果数据集不在指定目录中，则允许下载数据集
    transform=ToTensor()  # 应用转换操作，将图像数据转换为 PyTorch 张量
)

# 加载 FashionMNIST 测试数据集
test_data = datasets.FashionMNIST(
    root="data",          # 指定存储数据集的目录，如果数据集不存在则会下载到这个目录
    train=False,          # 表示我们正在加载测试集部分
    download=True,        # 如果数据集不在指定目录中，则允许下载数据集
    transform=ToTensor()  # 应用转换操作，将图像数据转换为 PyTorch 张量
)

```

这段代码使用 `torchvision` 的 `datasets` 模块来下载并加载 FashionMNIST 数据集。FashionMNIST 是 Zalando 文章图片的数据集，包含 60,000 个训练样本和 10,000 个测试样本。每个样本包括一张 28x28 的灰度图像及其对应的 10 个类别中的一个标签。我们将会在这一章节围绕FashionMNIST数据集进行整个项目的构建。使用官方提供的数据集的优点是它们通常都经过了良好的预处理，并且有标准的数据加载方式，非常适合快速开始一个项目或用于基准测试。

## 使用ImageFolder读取标准化数据

下面，我们以常见的 cifar10 数据集为例，演示如何使用 `PyTorch` 内置的 `ImageFolder` 类来构建 Dataset，这种方法非常适合处理那些已经按类别组织好并存储在不同文件夹下的图像数据集。首先，我们需要导入必要的库，并定义数据转换操作，这些操作能够帮助我们在加载图像时进行预处理和数据增强：

```python
# 导入torchvision 中必要的包
from torchvision import datasets, transforms

# 定义数据预处理操作
data_transform = transforms.Compose([
    transforms.RandomHorizontalFlip(),        # 随机水平翻转图像
    transforms.ToTensor(),                    # 将图像转换为 PyTorch 张量
    transforms.Normalize((0.5, 0.5, 0.5),     # 对张量图像的每个通道进行标准化
                        (0.5, 0.5, 0.5))      # 使用均值为 0.5，标准差为 0.5 进行标准化
])

# 指定训练数据集和验证数据集的文件路径
train_path = 'path_to_train_data'             # 训练数据集的路径
val_path = 'path_to_validation_data'          # 验证数据集的路径

# 使用 ImageFolder 类创建训练数据集
train_data = datasets.ImageFolder(root=train_path, transform=data_transform)
# 使用 ImageFolder 类创建验证数据集
val_data = datasets.ImageFolder(root=val_path, transform=data_transform)

```

在这段代码中，`ImageFolder` 类期望每个类别的图像都存放在单独的文件夹中。例如，所有属于类别 "dog" 的图片应该放在一个名为 "dog" 的子文件夹中（train_path和val_path对应图片存放的目录，目录下包含若干子目录，每个子目录对应属于同一个类的图片）。

`data_transform` 是一个 `transforms.Compose` 对象，它将多个图像变换步骤组合在一起。在这个例子中，我们首先对图像进行随机大小和比例的裁剪，然后进行随机水平翻转，最后将图像转换为 PyTorch 张量并进行标准化。这些变换不仅增加了数据的多样性，还有助于提高模型的泛化能力。这里我们会在下一章通过实战加以介绍并在[notebook](https://github.com/datawhalechina/thorough-pytorch/tree/main/notebook/%E7%AC%AC%E5%85%AB%E7%AB%A0%20PyTorch%E7%94%9F%E6%80%81%E7%AE%80%E4%BB%8B)中做了示例代码。虽然使用 PyTorch 内置的 `ImageFolder` 类可以方便地加载按文件夹分类的图像数据集，但这种方法也有其局限性。主要的缺点包括：

1. **固定的文件结构要求：** `ImageFolder` 要求数据必须按照特定的目录结构组织，即每个类别的图像存储在单独的文件夹中。这在现实世界的数据集中并不总是可行的，特别是当数据来源多样或者目录结构不规则时。
2. **有限的数据处理能力：** `ImageFolder` 通常与 `transforms` 一起使用，但对于复杂的数据处理流程来说，内置的变换可能不够用。例如，如果你需要根据图像的内容做出复杂的决策或者进行多模态数据的同步处理，`transforms` 可能不足以满足需求。
3. **缺少灵活性：** 如果你的数据集中包含额外的信息，比如文本描述、不同的标注格式或者辅助文件，使用 `ImageFolder` 可能就不够灵活了，因为它主要是为了处理标准图像文件夹设计的。

## 自定义Dataset完成数据的读取

在深度学习项目中，我们可能会遇到需要从非标准数据源读取数据的情况。当内置的 `Dataset` 类不满足需求时，PyTorch 提供了扩展的灵活性，允许我们继承 `torch.utils.data.Dataset` 创建自定义的 `Dataset`。这样做可以让我们根据项目的具体需求来加载和预处理数据。例如，如果你的数据集是通过 CSV 文件列出图像路径和对应标签，或者需要一些定制化的图像处理，那么你就需要使用自定义的 `Dataset`。自定义的 `Dataset` 类需要重载三个核心方法：

- `__init__`: 初始化函数，在创建类的实例时调用，用于接收并处理传入的参数，同时确定数据集的组成。
- `__getitem__`: 索引函数，使得实例对象可以像列表一样通过索引来获取数据，此方法需要返回模型训练或验证时所需的数据项。
- `__len__`: 返回数据集中的样本数，使得 len(dataset) 能够返回整个数据集的大小。

### 以CSV存储文件路径和标签

假设我们现在的数据将图片的名称和图片的标签存在了一个以`csv`为后缀的文件里，如下所示

```csv
tshirt1.jpg, 0
tshirt2.jpg, 0
......
ankleboot999.jpg, 9
```

每一行包含一个图像文件名和一个与之对应的标签，这是一种比较常见的图像分类的数据存储方式。接下来，我们将定义一个自定义的 `Dataset` 类，用于处理上述 CSV 文件格式的数据。

### 自定义Dataset

```python
# 导入必要的包
import os
import pandas as pd
from torch.utils.data import Dataset  
from torchvision.io import read_image

class CustomImageDataset(Dataset):
    def __init__(self, annotations_file, img_dir, transform=None, target_transform=None):
       	"""  
        初始化自定义的 Dataset。  
  
        Args:  
            annotations_file (str): 注解文件的路径，包含图像文件名和标签。  
            img_dir (str): 包含所有图像的目录路径。  
            transform (callable, optional): 一个函数，对每个图像进行变换。  
            target_transform (callable, optional): 一个函数，对每个标签进行变换。  
        """  
        self.img_labels = pd.read_csv(annotations_file)  # 读取注解文件  
        self.img_dir = img_dir  # 图像存放目录  
        self.transform = transform  # 变换函数  
        self.target_transform = target_transform  # 标签变换函数  

    def __len__(self):
        # 返回数据集中的样本数 
        return len(self.img_labels)

    def __getitem__(self, idx):  
        """  
        根据索引 idx 返回一个样本。  

        Args:  
            idx (int): 索引值  
  
        Returns:  
            tuple: (image, label) 图像及其标签  
        """  
        img_path = os.path.join(self.img_dir, self.img_labels.iloc[idx, 0])  # 获取图像路径  
        image = read_image(img_path)  # 读取图像  
        label = self.img_labels.iloc[idx, 1]  # 获取标签  
        if self.transform:  
            image = self.transform(image)  # 应用图像变换  
        if self.target_transform:  
            label = self.target_transform(label)  # 应用标签变换  
        return image, label 
```
在上面的代码中，`CustomImageDataset` 类从 `Dataset` 类继承，并实现了必要的 `__init__`, `__len__`, 和 `__getitem__` 方法。这允许我们定义如何读取图像文件和标签，以及如何对它们应用变换。

### 构建DataLoader读取数据

定义好自定义 `Dataset` 后，我们便可以利用 `DataLoader` 来实现高效的数据加载和批处理实现代码如下：

```python
from torch.utils.data import DataLoader

# 设置批量大小和工作进程数
batch_size = 32  # 定义每个数据批次的大小。即每次传递给模型的样本数量。
num_workers = 4  # 定义用于数据加载的子进程数。在Windows系统上通常设置为0，其他操作系统可以设置为其他值。

# 创建 DataLoader 实例用于训练数据
# DataLoader 是 PyTorch 中用于包装数据集的工具，使其可以返回批量数据，并提供数据打乱和多进程加载等功能。
train_loader = DataLoader(
    train_data,  # 指定要加载的数据集
    batch_size=batch_size,  # 设置每个批次加载多少个样本
    num_workers=num_workers,  # 设置 DataLoader 使用多少个子进程来加载数据
    shuffle=True,  # 指定是否在每个 epoch 开始时打乱数据（对于训练数据来说，通常会设置为 True）
    drop_last=True  # 指定是否丢弃最后一个不完整的批次（如果数据集大小不能被批量大小整除的话）
)

# 创建 DataLoader 实例用于验证数据
# 验证集 DataLoader 的设置与训练集类似，但通常不需要打乱数据（shuffle=False）。
val_loader = DataLoader(
    val_data,  # 指定要加载的数据集
    batch_size=batch_size,  # 设置每个批次加载多少个样本
    num_workers=num_workers,  # 设置 DataLoader 使用多少个子进程来加载数据
    shuffle=False  # 指定是否在每个 epoch 开始时打乱数据（对于验证数据来说，通常会设置为 False）
)

```

DataLoader包含很多的参数，在我们这个例子中使用到的参数有:

- batch_size：样本是按“批”读入的，每个批次加载的样本数量。`batch_size` 的选择通常受限于你的模型复杂度和机器的内存限制。较大的 `batch_size` 可以提高内存利用率和训练速度，但也可能导致内存溢出。此外，较大的 `batch_size` 有时会影响模型训练的收敛性和最终性能。
- num_workers：用于数据加载的子进程数。在Windows下设置为0，Linux下可以设置为非0的正数等。这个参数取决于你的机器的 CPU 核心数以及你正在处理的数据大小。增加 `num_workers` 可以使得数据加载更加平行化，但也会增加内存的消耗。设置为0意味着数据将在主进程中加载
- shuffle：这个参数决定我们是否打乱数据，对于训练数据，通常设置为 `True` 以减少模型对数据顺序的依赖，从而提高模型的泛化能力。验证和测试数据集通常不需要打乱，设置为`False`即可。
- drop_last：如果数据集的样本数不能被 `batch_size` 整除，`drop_last` 决定是否丢弃最后一个不完整的批次。在训练时，如果你不希望最后一个批次因为样本数不足而影响梯度计算，可以设置为 `True`。在测试时，通常希望评估所有数据，因此设置为 `False`。
- pin_memory: 如果设置为 `True`，数据加载器会在返回前将数据复制到 CUDA 固定内存中，当我们使用 GPU 训练时，设置 `pin_memory=True` 可以使得数据转移到 GPU 的速度变快。
- Dataloader还包括了很多的参数，但是我们常用的一些参数就是上面的这些例子，我们将会在后面的章节中，介绍更多的参数的意义以及用途。同时，当我们选择 DataLoader 的参数时，我们需要考虑数据集大小、内存限制、是否使用 GPU、以及你希望的加载效率。一般来说，我们需要多次实验才可能找到最佳的参数组合。通常，开始时使用默认参数，然后根据实际运行情况逐步调整，直到找到一个合适的平衡点。记住，每个数据集和项目可能需要不同的参数设置，因此没有一成不变的规则。

### 可视化读取的数据

在 PyTorch 中，`DataLoader` 返回的是一个迭代器，它允许我们遍历整个数据集并按批次获取数据。我们构建的`Dataset`每一次返回的都是data和target，每次从 `DataLoader` 迭代器中获取数据时，它都会返回一个包含两个主要元素的批次（batch），具体来说，当我们从 `DataLoader` 中获取一个批次时，我们会得到一个类似于 `(data, target)` 的元组，其中 `data` 是特征数据的张量，而 `target` 是对应的标签或目标张量。这里我们可以通过`next`和`iter`看一下我们的加载的数据。

```python
import matplotlib.pyplot as plt

# 从验证集的 DataLoader 中获取第一个批次的数据
images, labels = next(iter(val_loader))

# 打印当前批次图像的维度信息
print(f"Batch shape: {images.shape}")  # 输出类似于 (batch_size, channels, height, width)

# 可视化第一张图像
# PyTorch 中图像张量的维度顺序为 [C, H, W]，而绘图需要 [H, W, C]，所以需要转置
# plt.imshow(images[0].permute(1, 2, 0))  # 使用 permute 方法重新排列维度
# plt.show()  # 显示图像

# 注意: 如果图像的像素值范围是 [0, 1]，直接绘制即可；如果是 [0, 255]，需要先除以 255。
```



## 参考资料

1. [PyTorch datasets & dataloaders tutorial](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html)
2. [Writing custom datasets, dataloaders and transforms](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)
3. [A detailed example of how to generate your data in parallel with PyTorch](https://stanford.edu/~shervine/blog/pytorch-how-to-generate-data-parallel)
4. [An Introduction to Datasets and DataLoader in PyTorch](https://wandb.ai/sauravmaheshkar/Dataset-DataLoader/reports/An-Introduction-to-Datasets-and-DataLoader-in-PyTorch--VmlldzoxMDI5MTY2)
5. ChatGPT
